{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using CUDAnative, CUDAdrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using CUDAnative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (done) @0x00007f24f73b5cf0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Int64,1}:\n",
       " 6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs([(\"root@10.4.1.4:6666\",1)], tunnel=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function kernel_dist(X::AbstractVector{Float32}, Y::AbstractVector{Float32}, out::AbstractVector{Float32})\n",
    "    #Thread index\n",
    "    #Need to do the n-1 dance, since CUDA expects 0 and Julia does 1-indexing\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    out[i] = (X[i]-0.5)^2 + (Y[i]-0.5)^2\n",
    "    #Return nothing, since weâ€™re writing directly to the out array allocated on GPU\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function distmontegpu()\n",
    "    samples = 50000000\n",
    "    a = rand(Float32, (samples))\n",
    "    b = rand(Float32, (samples));\n",
    "    a_cu = CuArray(a)\n",
    "    b_cu = CuArray(b)\n",
    "    c_cu = similar(a_cu);\n",
    "    n = length(a)\n",
    "\n",
    "    ctx = CuCurrentContext()\n",
    "    dev = device(ctx)\n",
    "    max_threads = attribute(dev, CUDAdrv.MAX_THREADS_PER_BLOCK)\n",
    "    threads = min(max_threads, n)\n",
    "    blocks = ceil(Int, n/threads)\n",
    "    \n",
    "    @cuda (blocks, threads) kernel_dist(a_cu, b_cu, c_cu)\n",
    "    c = Array(c_cu)\n",
    "    destroy!(ctx)\n",
    "    c = Array(c_cu)\n",
    "    pi = 4*count(x->x<0.25,c)/length(c)\n",
    "    return pi\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere distmontegpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
