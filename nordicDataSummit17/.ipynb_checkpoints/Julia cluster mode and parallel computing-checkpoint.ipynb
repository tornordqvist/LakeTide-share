{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CPU: Multi-process on multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running on one core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function calc_pi(samples)\n",
    "    counter = 0\n",
    "    for i in 1:samples\n",
    "        x, y = rand(2)\n",
    "        if (x^2 + y^2 <=1)\n",
    "            counter += 1\n",
    "        end\n",
    "    end\n",
    "    π = 4 * counter / samples\n",
    "    return π\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1e8\n",
    "@time calc_pi(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of processes\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check workers\n",
    "workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear workers on all hosts\n",
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding more processes is a one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some more on another host\n",
    "#addprocs()\n",
    "addprocs([(\"root@10.4.1.4:6666\",2), (\"10.4.1.5:6666\",2)], sshflags=`-i id_rsa`, tunnel=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on multiple cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function parallel_calc_pi(samples)\n",
    "    counter = @parallel (+) for i=1:samples\n",
    "        x, y = rand(2)\n",
    "        return ifelse(x^2 + y^2 <= 1, 1, 0)\n",
    "    end\n",
    "    π = 4 * counter / samples\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time parallel_calc_pi(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDAnative, CUDAdrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kernel_dist (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function kernel_dist(X::AbstractVector{Float32}, Y::AbstractVector{Float32}, out::AbstractVector{Float32})\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    out[i] = (X[i]-0.5)^2 + (Y[i]-0.5)^2\n",
    "    #Return nothing, since we’re writing directly to the out array allocated on GPU\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9766"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 10000000\n",
    "a = rand(Float32, (samples))\n",
    "b = rand(Float32, (samples));\n",
    "a_cu = CuArray(a)\n",
    "b_cu = CuArray(b)\n",
    "c_cu = similar(a_cu);\n",
    "n = length(a)\n",
    "\n",
    "ctx = CuCurrentContext()\n",
    "dev = device(ctx)\n",
    "max_threads = attribute(dev, CUDAdrv.MAX_THREADS_PER_BLOCK)\n",
    "threads = min(max_threads, n)\n",
    "blocks = ceil(Int, n/threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "9766\n"
     ]
    }
   ],
   "source": [
    "println(\"$max_threads\\n$threads\\n$blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda (blocks, threads) kernel_dist(a_cu, b_cu, c_cu)\n",
    "c = Array(c_cu)\n",
    "\n",
    "destroy!(ctx)\n",
    "\n",
    "c = Array(c_cu)\n",
    "pi = 4*count(x->x<0.25,c)/length(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On GPU's located on multiple machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Int64,1}:\n",
       " 2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs([(\"root@10.4.1.4:6666\",1)], sshflags=`-i id_rsa`, tunnel=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using CUDAnative, CUDAdrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function kernel_dist(X::AbstractVector{Float32}, Y::AbstractVector{Float32}, gpu_cu::AbstractVector{Float32})\n",
    "    #Thread index\n",
    "    #Need to do the n-1 dance, since CUDA expects 0 and Julia does 1-indexing\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    gpu_cu[i] = (X[i]-0.5)^2 + (Y[i]-0.5)^2\n",
    "    #Return nothing, since we’re writing directly to the out array allocated on GPU\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function distmontegpu()\n",
    "    samples = 50000000\n",
    "    a = rand(Float32, (samples))\n",
    "    b = rand(Float32, (samples));\n",
    "    a_cu = CuArray(a)\n",
    "    b_cu = CuArray(b)\n",
    "    c_cu = similar(a_cu);\n",
    "    n = length(a)\n",
    "\n",
    "    ctx = CuCurrentContext()\n",
    "    dev = device(ctx)\n",
    "    max_threads = attribute(dev, CUDAdrv.MAX_THREADS_PER_BLOCK)\n",
    "    threads = min(max_threads, n)\n",
    "    blocks = ceil(Int, n/threads)\n",
    "    \n",
    "    @cuda (blocks, threads) kernel_dist(a_cu, b_cu, c_cu)\n",
    "    c = Array(c_cu)\n",
    "    destroy!(ctx)\n",
    "    c = Array(c_cu)\n",
    "    pi = 4*count(x->x<0.25,c)/length(c)\n",
    "    return pi\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere distmontegpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @parallel - The go-to tool for handling small tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs([(\"root@10.4.1.4:6666\", 1)], tunnel=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "tic()\n",
    "for i in 1:200000000\n",
    "    sum += i\n",
    "end\n",
    "toc()\n",
    "println(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "sum = @parallel (+) for i = 1:200000000\n",
    "    Int(i)\n",
    "end\n",
    "toc()\n",
    "println(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (daaaaaaaaaaamn!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
